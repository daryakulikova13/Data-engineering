# Проект реализуется в рамках дисциплины "Инжиниринг управления данными (базовый уровень)"
## Проект посвящён анализу белков, входяющих в геном микроорганизма *Pseudomonas aeruginosa*
<img width="4107" height="1928" alt="image" src="https://github.com/user-attachments/assets/cbbe11c0-4ac5-4032-8706-57dbed6e2592" />

### Актуальность:
*Pseudomonas aeruginosa* -условно-патогенная бактерия, обладающая значительным клиническим и научным потенциалом. Это грамотрицательная палочка, широко распространенная в окружающей среде, которая отличается исключительной способностью к выживанию в различных, в том числе малоблагоприятных, условиях. Её природная устойчивость к многим антибиотикам и дезинфицирующим средствам обусловлена низкой проницаемостью её внешней мембраны и наличием эффективных систем активного вывода токсинов. В клинической практике P. aeruginosa представляет серьезнейшую угрозу, особенно для пациентов с ослабленным иммунитетом. Она является частым возбудителем нозокомиальных (внутрибольничных) инфекций, вызывая такие заболевания, как пневмония (у пациентов на ИВЛ), инфекции кровотока, ожоговых ран, а также хронические инфекции дыхательных путей у больных муковисцидозом, где образование биопленок делает ее практически неуязвимой для терапии.

Именно сложность лечения инфекций, вызванных этой бактерией, и определяет огромную значимость изучения её протеома — полного набора белков. Протеомные данные позволяют расшифровать молекулярные механизмы патогенности и устойчивости. Анализируя белки, мы можем идентифицировать конкретные факторы вирулентности (например, экзотоксин А, протеазы, система секреции III типа), понять механизмы формирования биопленки и изучить работу насосов, выводящих антибиотики. Кроме того, протеомика является ключом к разработке новых методов диагностики и лечения: она позволяет обнаруживать белки-маркеры для быстрой идентификации штаммов, определять мишени для новых лекарств и вакцин, а также понимать, как бактерия адаптирует свой белковый профиль в ответ на действие антимикробных препаратов, что открывает пути к преодолению резистентности. Таким образом, глубокое изучение протеома Pseudomonas aeruginosa является не просто академическим интересом, а насущной необходимостью для разработки эффективных стратегий борьбы с этим опасным патогеном.


### Ссылка на датасет
Ссылка на датасет на Google Диске:[Открыть датасет](https://drive.google.com/drive/folders/17_n1YnmEWkbr0EwFk1wTnYzYauQCbaod?hl=ru)

### Структура проекта
1. Датасет, содержащий 8 столбцов и 1000 строк (формат .csv);
- Размер данных: (1000, 8)
- Колонки:`['ID', 'Name', 'Sequence', 'Molecular_Weight', 'Isoelectric_Point', 'Protein_Length', 'Amino_Acid_Composition', 'Hydrophobicity']`

2. Скрипт, позволяющий выгружать датасет из Google Drive и выводить первые десять строк данных (`data_loader.py`) [Результат выполнения скрипта](screenshot.jpg);
Первые 10 строк:
<table>
  <thead>
    <tr>
      <th>ID</th>
      <th>Name</th>
      <th>Sequence</th>
      <th>Molecular_Weight</th>
      <th>Isoelectric_Point</th>
      <th>Protein_Length</th>
      <th>Hydrophobicity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>WP_369686368.1</td>
      <td>ATP-binding cass...</td>
      <td>MLELNFTQTLGSHTLT...</td>
      <td>5756.5430</td>
      <td>8.517644</td>
      <td>56</td>
      <td>0.339286</td>
    </tr>
    <tr>
      <td>WP_369686367.1</td>
      <td>aldehyde dehydro...</td>
      <td>MQSRDNGKPLAEARGL...</td>
      <td>6617.5065</td>
      <td>6.106918</td>
      <td>62</td>
      <td>-0.146774</td>
    </tr>
    <tr>
      <td>WP_369686366.1</td>
      <td>hypothetical pro...</td>
      <td>GGEYLEIIEAARDIRV...</td>
      <td>9303.2892</td>
      <td>4.533444</td>
      <td>81</td>
      <td>-0.406173</td>
    </tr>
    <tr>
      <td>WP_369686365.1</td>
      <td>hypothetical pro...</td>
      <td>NAVVNQKRVPLAPNGD...</td>
      <td>6304.0708</td>
      <td>9.989715</td>
      <td>58</td>
      <td>-0.591379</td>
    </tr>
    <tr>
      <td>WP_369686364.1</td>
      <td>homocysteine S-m...</td>
      <td>MAGYLPQWLDAGAKLI...</td>
      <td>3619.1997</td>
      <td>7.810425</td>
      <td>34</td>
      <td>0.141176</td>
    </tr>
    <tr>
      <td>WP_369686363.1</td>
      <td>hypothetical pro...</td>
      <td>NRLILSPMGVRDVFRA...</td>
      <td>9303.6417</td>
      <td>8.358213</td>
      <td>83</td>
      <td>-0.059036</td>
    </tr>
    <tr>
      <td>WP_369686362.1</td>
      <td>DUF1043 family p...</td>
      <td>MTWEYALIGLVVGIII...</td>
      <td>5601.4576</td>
      <td>6.140907</td>
      <td>48</td>
      <td>-0.164583</td>
    </tr>
    <tr>
      <td>WP_369686361.1</td>
      <td>hypothetical pro...</td>
      <td>EIIKELVLRRKLFFKD...</td>
      <td>9935.1238</td>
      <td>5.608610</td>
      <td>83</td>
      <td>-0.630120</td>
    </tr>
    <tr>
      <td>WP_369686360.1</td>
      <td>WYL domain-conta...</td>
      <td>MQALLPCESPAALSIP...</td>
      <td>8771.2230</td>
      <td>8.802336</td>
      <td>81</td>
      <td>0.319753</td>
    </tr>
    <tr>
      <td>WP_369686359.1</td>
      <td>fimbrial protein...</td>
      <td>IIPFTCQTPDVIVPMG...</td>
      <td>9075.0983</td>
      <td>6.224745</td>
      <td>85</td>
      <td>-0.192941</td>
    </tr>
  </tbody>
</table>

3. Выполнено приведение типов переменных и создан файл `pseudomonas_aeruginosa.parquet`;
<img width="430" height="258" alt="image" src="https://github.com/user-attachments/assets/d90fa480-9446-4137-9fe5-2b6a73f9436e" />

4. Переменные из `pseudomonas_aeruginosa.parquet` были загружены в базу данных homeworks (скрипт выполнения находится в файле `write_to_db.py`);

5. EDA-отчёт в виде Jypiter Notebook, где проведена базовая статистическая обработка данных, а также частота распределения признаков и корреляционный анализ.

Рендер на Jypiter Notebook - [![View on nbviewer](https://img.shields.io/badge/render-nbviewer-orange.svg)](https://nbviewer.org/github/daryakulikova13/Data-engineering/blob/main/notebooks/Seaborn_EDA.ipynb)

6. Создан  ETL-процесс:

```
Data-engineering/
├── etl/                         # Пакет ETL
│   ├── __init__.py              # Инициализация пакета
│   ├── extract.py               # Извлечение данных
│   ├── transform.py             # Трансформация данных  
│   ├── load.py                  # Загрузка данных
│   └── main.py                  # Запуск процесса
└── README.md                    # Документация к проекту

```
**ETL (Extract, Transform, Load)** — это фундаментальный процесс в области управления данными, который представляет собой три ключевых этапа работы с информацией: извлечение, преобразование и загрузку. В контексте данного проекта ETL процесс был реализован для обработки данных о белках бактерии *Pseudomonas aeruginosa* — условно-патогенного микроорганизма, имеющего важное значение в медицинских и биотехнологических исследованиях.

**Этап Extract (Извлечение данных)**
Процесс начинается с этапа извлечения данных из внешнего источника. В нашем случае источником данных выступает CSV файл, размещённый на Google Drive. Для доступа к файлу используется его уникальный идентификатор (file_id), который передаётся в качестве аргумента командной строки.

- Модуль **extract.py** выполняет несколько критически важных функций:
1. Устанавливает соединение с Google Drive через сформированный URL
2. Загружает данные с использованием библиотеки pandas
3. Проводит первичную валидацию структуры данных, проверяя наличие всех обязательных колонок
4.Сохраняет исходные ("сырые") данные для обеспечения возможности аудита и отслеживания изменений
Особенностью этого этапа является обработка потенциальных ошибок сети, невалидных идентификаторов файлов и проблем с форматом данных.

- В модуле **transform.py** реализован комплекс преобразований:
1. Приведение типов данных
2. Числовые колонки (Molecular_Weight, Isoelectric_Point, Hydrophobicity) преобразуются из строкового формата в числовой с плавающей точкой.
3. Колонки ID, Name и Sequence очищаются от лишних пробелов и приводятся к единому строковому формату. Это обеспечивает консистентность данных и предотвращает потенциальные ошибки при анализе.
4. Колонка Amino_Acid_Composition, содержащая данные в формате JSON-строки, преобразуется в словари Python, что значительно упрощает дальнейший анализ аминокислотного состава белков.
Структурные улучшения
5. Колонка Name, содержащая комбинированную информацию о названии белка и организме-источнике, разделяется на две отдельные колонки: protein_name (чистое название белка) и organism (организм, из которого выделен белок). Это преобразование существенно улучшает структуру данных для последующих запросов и фильтраций.

- Модуль **load.py** реализует два основных направления загрузки:
1. Загрузка в реляционную базу данных
Данные загружаются в PostgreSQL с использованием SQLAlchemy — мощной ORM библиотеки для Python. В соответствии с требованиями задания, количество загружаемых строк ограничивается 100 записями. Процесс включает приведение имён колонок к нижнему регистру для соответствия стандартам именования в SQL и использование стратегии полной замены данных (REPLACE) при каждой новой загрузке.
Сохранение в колоночном формате

- Модуль **main.py** выступает в роли оркестратора всего ETL процесса:
1. Обязательный параметр --file-id позволяет указывать различные источники данных
2. Опциональные параметры --table-name и --max-rows предоставляют контроль над результатом загрузки
3. Обработка ошибок и надежность
4. Валидация входных данных на соответствие ожидаемой структуре
5. Обработка исключений при преобразовании типов
6. Контроль качества выходных данных

**Заключение и практическая значимость:**
- Реализованный ETL процесс преобразует неструктурированные сырые данные о белках Pseudomonas aeruginosa в качественный, готовый к анализу dataset. Особую ценность представляет автоматизация процесса, что позволяет регулярно обновлять данные при поступлении новой информации без ручного вмешательства.
- Полученные данные могут быть использованы в различных биоинформатических исследованиях: анализ физико-химических свойств белков, изучение аминокислотного состава, сравнительный анализ белков разных организмов, а также в машинном обучении для предсказания свойств белков по их последовательности.
- Процесс демонстрирует лучшие практики работы с данными: воспроизводимость, документированность, обработка ошибок и гибкость конфигурации, что делает его надежным инструментом в арсенале исследователя данных.

**БЛАГОДАРЮ ЗА ВНИМАНИЕ К МОЕМУ ПРОЕКТУ!**

<img width="512" height="512" alt="image" src="https://github.com/user-attachments/assets/69e2efd6-fb51-489d-8fbb-770ed1c57997" />



